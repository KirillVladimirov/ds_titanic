{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://www.kaggle.com/zikazika/whats-the-fuss-with-machine-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T22:50:59.993070Z",
     "start_time": "2019-03-24T22:50:48.823136Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kirillvladimirov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kirillvladimirov/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/kirillvladimirov/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kirillvladimirov/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/kirillvladimirov/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kirillvladimirov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /Users/kirillvladimirov/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/kirillvladimirov/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "# for all NLP related operations on text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# To consume Twitter's API\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler \n",
    "\n",
    "# To identify the sentiment of text\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from textblob.np_extractors import ConllExtractor\n",
    "\n",
    "# ignoring all the warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# downloading stopwords corpus\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('brown')\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "# for showing all the plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T22:52:22.938307Z",
     "start_time": "2019-03-24T22:52:22.935128Z"
    }
   },
   "outputs": [],
   "source": [
    "# keys and tokens to access Twitter API\n",
    "consumer_key = 'Sec3MvclRIx2RVlgu9l0SJX6D'\n",
    "consumer_secret = 'ayoPNWtBm7fWpMBoK6EwRmegu3SW8Rw9mzJkottkv97quPe941'\n",
    "access_token = '736550752760406018-so5CPJrEbJKb3c3Pq8va3VFr0yk4S0E'\n",
    "access_token_secret = 'Cgr8tz0h6FTU7kxAjDzpHnjffNTHxWsBytXnu4Ihd1TFb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_client = TwitterClient()\n",
    "\n",
    "# calling function to get tweets\n",
    "tweets_df = twitter_client.get_tweets('Machine Learning', maxTweets=10000)\n",
    "print(f'tweets_df Shape - {tweets_df.shape}')\n",
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def underlying_atmosphere_textblob(text):\n",
    "    analysis = TextBlob(text)\n",
    "    if analysis.sentiment.polarity > 0: \n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity == 0: \n",
    "        return 'neutral'\n",
    "    else: \n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_using_textblob = tweets_df.tweets.apply(lambda tweet: underlying_atmosphere_textblob(tweet))\n",
    "pd.DataFrame(sentiments_using_textblob.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['sentiment'] = sentiments_using_textblob\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(text, pattern_regex):\n",
    "    r = re.findall(pattern_regex, text)\n",
    "    for i in r:\n",
    "        text = re.sub(i, '', text)\n",
    "    \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are keeping cleaned tweets in a new column called 'tidy_tweets'\n",
    "tweets_df['tidy_tweets'] = np.vectorize(remove_pattern)(tweets_df['tweets'], \"@[\\w]*: | *RT*\")\n",
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = []\n",
    "\n",
    "for index, row in tweets_df.iterrows():\n",
    "    # Here we are filtering out all the words that contains link\n",
    "    words_without_links = [word for word in row.tidy_tweets.split() if 'http' not in word]\n",
    "    cleaned_tweets.append(' '.join(words_without_links))\n",
    "\n",
    "tweets_df['tidy_tweets'] = cleaned_tweets\n",
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_df[tweets_df['tidy_tweets']!='']\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.drop_duplicates(subset=['tidy_tweets'], keep=False)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_df.reset_index(drop=True)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['absolute_tidy_tweets'] = tweets_df['tidy_tweets'].str.replace(\"[^a-zA-Z# ]\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = set(stopwords)\n",
    "cleaned_tweets = []\n",
    "\n",
    "for index, row in tweets_df.iterrows():\n",
    "    \n",
    "    # filerting out all the stopwords \n",
    "    words_without_stopwords = [word for word in row.absolute_tidy_tweets.split() if not word in stopwords_set and '#' not in word.lower()]\n",
    "    \n",
    "    # finally creating tweets list of tuples containing stopwords(list) and sentimentType \n",
    "    cleaned_tweets.append(' '.join(words_without_stopwords))\n",
    "    \n",
    "tweets_df['absolute_tidy_tweets'] = cleaned_tweets\n",
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweet = tweets_df['absolute_tidy_tweets'].apply(lambda x: x.split())\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [word_lemmatizer.lemmatize(i) for i in x])\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tokens in enumerate(tokenized_tweet):\n",
    "    tokenized_tweet[i] = ' '.join(tokens)\n",
    "\n",
    "tweets_df['absolute_tidy_tweets'] = tokenized_tweet\n",
    "tweets_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textblob_key_phrases = []\n",
    "extractor = ConllExtractor()\n",
    "\n",
    "for index, row in tweets_df.iterrows():\n",
    "    # filerting out all the hashtags\n",
    "    words_without_hash = [word for word in row.tidy_tweets.split() if '#' not in word.lower()]\n",
    "    \n",
    "    hash_removed_sentence = ' '.join(words_without_hash)\n",
    "    \n",
    "    blob = TextBlob(hash_removed_sentence, np_extractor=extractor)\n",
    "    textblob_key_phrases.append(list(blob.noun_phrases))\n",
    "\n",
    "textblob_key_phrases[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['key_phrases'] = textblob_key_phrases\n",
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(all_words):\n",
    "    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=100, relative_scaling=0.5, colormap='Dark2').generate(all_words)\n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join([text for text in tweets_df['absolute_tidy_tweets'][tweets_df.sentiment == 'positive']])\n",
    "generate_wordcloud(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join([text for text in tweets_df['absolute_tidy_tweets'][tweets_df.sentiment == 'negative']])\n",
    "generate_wordcloud(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join([text for text in tweets_df['absolute_tidy_tweets'][tweets_df.sentiment == 'neutral']])\n",
    "generate_wordcloud(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to collect hashtags\n",
    "def hashtag_extract(text_list):\n",
    "    hashtags = []\n",
    "    # Loop over the words in the tweet\n",
    "    for text in text_list:\n",
    "        ht = re.findall(r\"#(\\w+)\", text)\n",
    "        hashtags.append(ht)\n",
    "\n",
    "    return hashtags\n",
    "\n",
    "def generate_hashtag_freqdist(hashtags):\n",
    "    a = nltk.FreqDist(hashtags)\n",
    "    d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
    "                      'Count': list(a.values())})\n",
    "    # selecting top 15 most frequent hashtags     \n",
    "    d = d.nlargest(columns=\"Count\", n = 25)\n",
    "    plt.figure(figsize=(16,7))\n",
    "    ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n",
    "    plt.xticks(rotation=80)\n",
    "    ax.set(ylabel = 'Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = hashtag_extract(tweets_df['tidy_tweets'])\n",
    "hashtags = sum(hashtags, [])\n",
    "generate_hashtag_freqdist(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df_full_key_phrases = tweets_df[tweets_df['key_phrases'].str.len()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tfidf_word_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, stop_words='english')\n",
    "tfidf_word_feature = tfidf_word_vectorizer.fit_transform(tweets_df_full_key_phrases['tidy_tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_sents = tweets_df_full_key_phrases['key_phrases'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# TF-IDF for key phrases\n",
    "tfidf_phrase_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2)\n",
    "tfidf_phrase_feature = tfidf_phrase_vectorizer.fit_transform(phrase_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = tweets_df_full_key_phrases['sentiment'].apply(lambda x: 0 if x==\"negative\" else (1 if x==\"neutral\" else 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_model(X_train, X_test, y_train, y_test):\n",
    "    naive_classifier = LogisticRegression()\n",
    "    naive_classifier.fit(X_train.toarray(), y_train)\n",
    "\n",
    "    # predictions over test set\n",
    "    predictions = naive_classifier.predict(X_test.toarray())\n",
    "\n",
    "    # calculating Accuracy Score\n",
    "    print(f'Accuracy Score - {accuracy_score(y_test, predictions)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_word_feature, target_variable, test_size=0.2, random_state=123)\n",
    "naive_model(X_train, X_test, y_train, y_test)tweeter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
